{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data dan read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/allbrand.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_phising = open(\"data/data_phishing_37175.json\")\n",
    "data_phising = json.load(data_phising) \n",
    "data_legitimate = open(\"data/data_legitimate_36400.json\")\n",
    "data_legitimate = json.load(data_legitimate)\n",
    "with open('data/keywords.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "keywords = data.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word decomposer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_raw_extraction(domain, subdomain, path):\n",
    "    w_domain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", domain.lower())\n",
    "    w_subdomain = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", subdomain.lower())\n",
    "    w_path = re.split(\"\\-|\\.|\\/|\\?|\\=|\\@|\\&|\\%|\\:|\\_\", path.lower())\n",
    "\n",
    "    raw_words = w_domain + w_path + w_subdomain\n",
    "    #raw_words = list(set(raw_words))\n",
    "    raw_words = list(filter(None, raw_words))\n",
    "\n",
    "    return raw_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maliciouness Analysis Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "accepted_chars = 'abcdefghijklmnopqrstuvwxyz '\n",
    "\n",
    "pos = dict([(char, idx) for idx, char in enumerate(accepted_chars)])\n",
    "\n",
    "def normalize(line):\n",
    "    \"\"\" Return only the subset of chars from accepted_chars.\n",
    "    This helps keep the  model relatively small by ignoring punctuation,\n",
    "    infrequenty symbols, etc. \"\"\"\n",
    "    return [c.lower() for c in line if c.lower() in accepted_chars]\n",
    "\n",
    "def ngram(n, l):\n",
    "    \"\"\" Return all n grams from l after normalizing \"\"\"\n",
    "    filtered = normalize(l)\n",
    "    for start in range(0, len(filtered) - n + 1):\n",
    "        yield ''.join(filtered[start:start + n])\n",
    "\n",
    "def train():\n",
    "    \"\"\" Write a simple model as a pickle file \"\"\"\n",
    "    k = len(accepted_chars)\n",
    "    # Assume we have seen 10 of each character pair.  This acts as a kind of\n",
    "    # prior or smoothing factor.  This way, if we see a character transition\n",
    "    # live that we've never observed in the past, we won't assume the entire\n",
    "    # string has 0 probability.\n",
    "    counts = [[10 for i in range(k)] for i in range(k)]\n",
    "\n",
    "    # Count transitions from big text file, taken\n",
    "    # from http://norvig.com/spell-correct.html\n",
    "    for line in open('big.txt'):\n",
    "        for a, b in ngram(2, line):\n",
    "            counts[pos[a]][pos[b]] += 1\n",
    "\n",
    "    # Normalize the counts so that they become log probabilities.\n",
    "    # We use log probabilities rather than straight probabilities to avoid\n",
    "    # numeric underflow issues with long texts.\n",
    "    # This contains a justification:\n",
    "    # http://squarecog.wordpress.com/2009/01/10/dealing-with-underflow-in-joint-probability-calculations/\n",
    "    for i, row in enumerate(counts):\n",
    "        s = float(sum(row))\n",
    "        for j in range(len(row)):\n",
    "            row[j] = math.log(row[j] / s)\n",
    "\n",
    "    # Find the probability of generating a few arbitrarily choosen good and\n",
    "    # bad phrases.\n",
    "    good_probs = [avg_transition_prob(l, counts) for l in open('good.txt')]\n",
    "    bad_probs = [avg_transition_prob(l, counts) for l in open('bad.txt')]\n",
    "\n",
    "    # Assert that we actually are capable of detecting the junk.\n",
    "    assert min(good_probs) > max(bad_probs)\n",
    "\n",
    "    # And pick a threshold halfway between the worst good and best bad inputs.\n",
    "    thresh = (min(good_probs) + max(bad_probs)) / 2\n",
    "    pickle.dump({'mat': counts, 'thresh': thresh}, open('gib_model.pki', 'wb'))\n",
    "\n",
    "def avg_transition_prob(l, log_prob_mat):\n",
    "    \"\"\" Return the average transition prob from l through log_prob_mat. \"\"\"\n",
    "    log_prob = 0.0\n",
    "    transition_ct = 0\n",
    "    for a, b in ngram(2, l):\n",
    "        log_prob += log_prob_mat[pos[a]][pos[b]]\n",
    "        transition_ct += 1\n",
    "    # The exponentiation translates from log probs to probs.\n",
    "    return math.exp(log_prob / (transition_ct or 1))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Feature Extraction (40 Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pprint\n",
    "import pickle\n",
    "import pygtrie\n",
    "import requests\n",
    "\n",
    "from traceback import format_exc\n",
    "from word_with_nlp import nlp_class\n",
    "from word_splitter_file import WordSplitterClass\n",
    "\n",
    "from ns_log import NsLog\n",
    "\n",
    "\n",
    "class url_rules:\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"initializing\")\n",
    "\n",
    "        self.logger = NsLog(\"log\")\n",
    "        self.path_data = \"data/\"\n",
    "        self.name_brand_file = \"allbrand.txt\"\n",
    "#         self.path_alexa_files = \"data/alexa-tld/\"\n",
    "\n",
    "        self.nlp_manager = nlp_class()\n",
    "        self.pp = pprint.PrettyPrinter(indent=4)\n",
    "        self.word_splitter = WordSplitterClass()\n",
    "\n",
    "        allbrand_txt = open(\"{0}{1}\".format(self.path_data, self.name_brand_file), \"r\")\n",
    "        self.allbrand = self.__txt_to_list(allbrand_txt)\n",
    "\n",
    "\n",
    "    def __txt_to_list(self, txt_object):\n",
    "\n",
    "        list = []\n",
    "\n",
    "        for line in txt_object:\n",
    "            list.append(line.strip())\n",
    "\n",
    "        txt_object.close()\n",
    "\n",
    "        return list\n",
    "\n",
    "    def rules_main(self, domain, tld, subdomain, path, words_raw):\n",
    "\n",
    "        features = {}\n",
    "        info_nlp = {}\n",
    "\n",
    "        try:\n",
    "            features.update(self.digit_count(domain, subdomain, path))             # digitcount\n",
    "            features.update(self.length(domain, subdomain, path))                  # uzunluk\n",
    "            features.update(self.tld_check(tld))                                   # tld check\n",
    "            features.update(self.check_rule_5(words_raw))                          # www-com\n",
    "            features.update(self.punny_code(domain))                               # punnycode\n",
    "            features.update(self.random_domain(domain))                            # random_domain\n",
    "            features.update(self.subdomain_count(subdomain))                       # subdomain count\n",
    "            features.update(self.char_repeat(words_raw))                           # char_repeat\n",
    "#             features.update(self.alexa_check(domain, tld))                         # alexa1m  check\n",
    "            #features.update(self.alexa_trie(domain, tld))                         # alexa1m check trie\n",
    "            features.update(self.special_chars(domain, subdomain, path))           # - . / @\n",
    "            features.update(self.check_domain_in_list(domain))\n",
    "    \n",
    "            result_nlp = self.nlp_features(words_raw)\n",
    "            features.update(result_nlp['features'])                                # words_info\n",
    "    \n",
    "            info_nlp = result_nlp['info']\n",
    "        \n",
    "        except:\n",
    "            self.logger.error(\"url_rules.main() Error : {0}\".format(format_exc()))\n",
    "\n",
    "        return info_nlp, features\n",
    "\n",
    "    def digit_count(self, domain, subdomain, path):\n",
    "\n",
    "        result = {'domain_digit_count': 0,\n",
    "                  'subdomain_digit_count': 0,\n",
    "                  'path_digit_count': 0}\n",
    "\n",
    "        for letter in domain:\n",
    "            if letter.isdigit():\n",
    "                result['domain_digit_count'] = result['domain_digit_count'] + 1\n",
    "\n",
    "        for letter in subdomain:\n",
    "            if letter.isdigit():\n",
    "                result['subdomain_digit_count'] = result['subdomain_digit_count'] + 1\n",
    "\n",
    "        for letter in path:\n",
    "            if letter.isdigit():\n",
    "                result['path_digit_count'] = result['path_digit_count'] + 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def length(self, domain, subdomain, path):\n",
    "\n",
    "        domain_uzunluk = len(domain)\n",
    "        subdomain_uzunluk = len(subdomain)\n",
    "        path_uzunluk = len(path)\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        result['domain_length'] = domain_uzunluk\n",
    "        result['subdomain_length'] = subdomain_uzunluk\n",
    "        result['path_length'] = path_uzunluk\n",
    "\n",
    "        return result\n",
    "\n",
    "    def tld_check(self, tld):\n",
    "\n",
    "        common_tld = [\"com\", \"org\", \"net\", \"de\", \"edu\", \"gov\"]\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if tld in common_tld:\n",
    "            result[\"isKnownTld\"] = 1\n",
    "        else:\n",
    "            result[\"isKnownTld\"] = 0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def check_rule_5(self, words_raw):\n",
    "\n",
    "        result = {'www': 0, \"com\": 0}\n",
    "\n",
    "        for word in words_raw:\n",
    "            if not word.find('www') == -1:\n",
    "                result['www'] = result['www'] + 1\n",
    "\n",
    "            if not word.find('com') == -1:\n",
    "                result['com'] = result['com'] + 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def punny_code(self, line):\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if line.startswith(\"xn--\"):\n",
    "\n",
    "            result['punnyCode'] = 1\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            result['punnyCode'] = 0\n",
    "            return result\n",
    "\n",
    "    def random_domain(self, domain):\n",
    "\n",
    "        result = {'random_domain': self.nlp_manager.check_word_random(domain)}\n",
    "\n",
    "        return result\n",
    "\n",
    "    def subdomain_count(self, line):\n",
    "\n",
    "        sub = line.split(\".\")\n",
    "\n",
    "        result = {}\n",
    "        result['subDomainCount'] = len(sub)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __all_same(self, items):\n",
    "        return all(x == items[0] for x in items)\n",
    "\n",
    "    def char_repeat(self, words_raw):\n",
    "\n",
    "        result = {'char_repeat': 0}\n",
    "        repeat = {'2': 0, '3': 0, '4': 0, '5': 0}\n",
    "        part = [2, 3, 4, 5]\n",
    "\n",
    "        \"sliding window mantigi repeat sayisi kadar eleman al\" \\\n",
    "        \"hepsi ayni mi diye bak - ayni ise artir\"\n",
    "\n",
    "        for word in words_raw:\n",
    "            for char_repeat_count in part:\n",
    "                for i in range(len(word) - char_repeat_count + 1):\n",
    "                    sub_word = word[i:i + char_repeat_count]\n",
    "                    if self.__all_same(sub_word):\n",
    "                        repeat[str(char_repeat_count)] = repeat[str(char_repeat_count)] + 1\n",
    "\n",
    "        result['char_repeat'] = sum(list(repeat.values()))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def alexa_check(self, domain, tld):\n",
    "\n",
    "        is_find_tld = 0\n",
    "        is_find = 0\n",
    "        line = domain+\".\"+tld\n",
    "\n",
    "        letter = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\",\n",
    "                  \"n\", \"o\", \"p\", \"r\", \"s\", \"t\", \"u\", \"v\", \"y\", \"z\", \"w\", \"x\", \"q\",\n",
    "                  \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"]\n",
    "\n",
    "        try:\n",
    "            if line[0] in letter:\n",
    "                alexa_txt = open(\"{0}{1}.txt\".format(self.path_alexa_files, line[0]), \"r\")\n",
    "                alexaList_tld = []  #tldli\n",
    "                alexa_list = []  #tldsiz\n",
    "\n",
    "                for alexa_line in alexa_txt:\n",
    "                    alexaList_tld.append(alexa_line.strip())\n",
    "                    alexa_list.append(alexa_line.strip().split(\".\")[0])\n",
    "                alexa_txt.close()\n",
    "\n",
    "                for alexa_line in alexaList_tld:\n",
    "                    if line.strip() == alexa_line.strip():\n",
    "                        is_find_tld = 1\n",
    "                        break\n",
    "\n",
    "                for alexa_line in alexa_list:\n",
    "                    line_domain = line.split(\".\")[0]\n",
    "                    if line_domain.strip() == alexa_line.strip():\n",
    "                        is_find = 1\n",
    "                        break\n",
    "        except:\n",
    "            self.logger.debug(line + \"işlenirken hata uzunluktan dolayı\")\n",
    "            self.logger.error(\"url_rules.check_rule_11()-Alexa  /  Error : {0}\".format(format_exc()))\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if is_find_tld == 1:\n",
    "            result['alexa1m_tld'] = 1\n",
    "        else:\n",
    "            result['alexa1m_tld'] = 0\n",
    "\n",
    "        if is_find == 1:\n",
    "            result['alexa1m'] = 1\n",
    "        else:\n",
    "            result['alexa1m'] = 0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def alexa_trie(self, domain, tld):\n",
    "\n",
    "        line = domain+\".\"+tld\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        try:\n",
    "            #if self.alexa1mm[line[0].lower()].has_key(line):\n",
    "            if self.trie_alexa_tld.has_key(line):\n",
    "                result['alexa1m_tld_trie'] = 1\n",
    "            else:\n",
    "                result['alexa1m_tld_trie'] = 0\n",
    "\n",
    "            if self.trie_alexa_tldsiz.has_key(domain):\n",
    "                result['alexa1m_tldsiz_trie'] = 1\n",
    "            else:\n",
    "                result['alexa1m_tldsiz_trie'] = 0\n",
    "        except:\n",
    "            self.logger.debug(line + \"işlenirken alexa\")\n",
    "            self.logger.error(\"url_rules.check_rule_11()-Alexa  /  Error : {0}\".format(format_exc()))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def special_chars(self, domain, subdomain, path):\n",
    "\n",
    "        special_char = {'-': 0, \".\": 0, \"/\": 0, '@': 0, '?': 0, '&': 0, '=': 0, \"_\": 0}\n",
    "        special_char_letter = special_char.keys()\n",
    "\n",
    "        for l in domain:\n",
    "            if l in special_char_letter:\n",
    "                special_char[l] = special_char[l] + 1\n",
    "\n",
    "        for l in subdomain:\n",
    "            if l in special_char_letter:\n",
    "                special_char[l] = special_char[l] + 1\n",
    "\n",
    "        for l in path:\n",
    "            if l in special_char_letter:\n",
    "                special_char[l] = special_char[l] + 1\n",
    "\n",
    "        return special_char\n",
    "\n",
    "    def check_domain_in_list(self, domain):\n",
    "\n",
    "        result = {}\n",
    "        if domain in self.allbrand:\n",
    "            result['domain_in_brand_list'] = 1\n",
    "        else:\n",
    "            result['domain_in_brand_list'] = 0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def nlp_features(self, words_raw):\n",
    "\n",
    "        \"\"\"\n",
    "        keywords_in_words, brands_in_words,\n",
    "        dga_in_words, len_lt_7, len_gt_7 \n",
    "        \"\"\"\n",
    "        grouped_words = self.nlp_manager.parse(words_raw)\n",
    "\n",
    "        splitted_words = self.word_splitter._splitl(grouped_words['len_gt_7'])\n",
    "        \"\"\"\n",
    "        found_keywords, found_brands,\n",
    "        similar_to_keyword, similar_to_brand,\n",
    "        other_words, target_words\n",
    "        \"\"\"\n",
    "\n",
    "        fraud_analyze_result = self.nlp_manager.fraud_analysis(grouped_words, splitted_words)\n",
    "\n",
    "        result = self.nlp_manager.evaluate(grouped_words, fraud_analyze_result, splitted_words)\n",
    "        split = {'raw': grouped_words['len_gt_7'], 'splitted': splitted_words}\n",
    "        result['info']['compoun_words'] = split\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules extraction (Ubah NLP  Feature jadi dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from traceback import format_exc\n",
    "from ns_log import NsLog\n",
    "from url_rules import url_rules\n",
    "from active_rules import active_rules\n",
    "\n",
    "\n",
    "class rule_extraction:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = NsLog(\"log\")\n",
    "        self.url_rules_o = url_rules()\n",
    "        self.active_rules_o = active_rules()\n",
    "\n",
    "    def extraction(self, parsed_domains):\n",
    "\n",
    "        self.logger.info(\"rule_extraction.extraction() is running\")\n",
    "\n",
    "        self.parser_object = domain_parser()\n",
    "        domain_features = []\n",
    "        try:\n",
    "            for line in tqdm(parsed_domains):  # self.bar(parsed_domains)\n",
    "                info = line\n",
    "\n",
    "                #  info['mail'] = 'whoisden cekilecek'\n",
    "                nlp_info, url_features = self.url_rules_o.rules_main(info['domain'],\n",
    "                                                                     info['tld'],\n",
    "                                                                     info['subdomain'],\n",
    "                                                                     info['path'],\n",
    "                                                                     info['words_raw'])  # url kurallarin calistigi yer\n",
    "\n",
    "                info['nlp_info'] = nlp_info\n",
    "                info['nlp_info']['words_raw'] = info['words_raw']\n",
    "                info.pop(\"words_raw\", None)\n",
    "\n",
    "              #  domain_info, dns_features = self.dns_rules_o.rules_main(line_lst)  # dns rules\n",
    "\n",
    "                outputDict = {}\n",
    "\n",
    "              #  info['dns_records'] = domain_info\n",
    "\n",
    "                outputDict['info'] = info\n",
    "                outputDict['url_features'] = url_features\n",
    "\n",
    "              #  outputDict['dns_features'] = dns_features\n",
    "\n",
    "                domain_features.append(outputDict)\n",
    "\n",
    "            #domain_features = self.active_rules_o.goog_safe_browsing(domain_features)  # active kuralların çalıştığı yer\n",
    "        except:\n",
    "            self.logger.error(\"Error : {0}\".format(format_exc()))\n",
    "\n",
    "        return domain_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubah dari dictionary jadi JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain_parser import domain_parser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(open(\"parsed_domain_list.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-31 01:39:53,906 - INFO - rule_extraction.extraction() is running\n",
      "  0%|                                                                                        | 0/73575 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 73575/73575 [51:06<00:00, 24.00it/s]\n"
     ]
    }
   ],
   "source": [
    "rules = rule_extraction()\n",
    "extracted_rules = rules.extraction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"extracted_rules.json\", \"w\")\n",
    "file.write(json.dumps(extracted_rules))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert json ke arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_train(features, param):\n",
    "    # arff convert header\n",
    "    features_keys_active = []\n",
    "    features_keys_url = list()\n",
    "    \n",
    "    ArffStr = '''@relation weka-test\\n\\n'''\n",
    "    features_keys_url = list(features[0]['url_features'].keys())\n",
    "\n",
    "    if param == '-a':\n",
    "        features_keys_active = list(features[0]['active_features'].keys())\n",
    "\n",
    "    for line in features_keys_url:\n",
    "        ArffStr = ArffStr + '@attribute ' + line + \" numeric\\n\"\n",
    "\n",
    "    if param == '-a':\n",
    "        for line in features_keys_active:\n",
    "            ArffStr = ArffStr + '@attribute ' + line + \" numeric\\n\"\n",
    "\n",
    "    ArffStr = ArffStr + '@attribute class {phising, legitimate}' + \"\\n\\n@data\\n\"\n",
    "    # header son\n",
    "\n",
    "\n",
    "    for each_domain in features:\n",
    "        tmp = \"\"\n",
    "\n",
    "        for key in features_keys_url:\n",
    "            tmp = tmp + str(each_domain['url_features'][key])+\",\"\n",
    "\n",
    "        if param == '-a':\n",
    "            for key_a in features_keys_active:\n",
    "                tmp = tmp + str(each_domain['active_features'][key_a]) + \",\"\n",
    "\n",
    "        tmp = tmp + each_domain['info']['class']+\"\\n\"\n",
    "        ArffStr = ArffStr + tmp\n",
    "\n",
    "    return ArffStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arff_str = convert_for_train(extracted_rules, '') # todo active_features icin -a param girilecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"features_1.arff\", \"w\")\n",
    "file.write(arff_str)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data NLP feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "target = []\n",
    "\n",
    "file = \"features_NLP.arff\"\n",
    "\n",
    "train_dataset, train_meta = arff.loadarff(open(file, \"r\"))\n",
    "train = train_dataset[train_meta.names()[:-1]]  # everything but the last column\n",
    "target = train_dataset[train_meta.names()[len(train_meta.names()) - 1]]  # last column\n",
    "train = np.asarray(train.tolist(), dtype=np.float32)  # olay burda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tes cross validation 10 fold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.6388724888933339\n",
      "Precision :  0.7436953007424807\n",
      "Recall :  0.6388724888933339\n",
      "F1 :  0.5965821045834743\n"
     ]
    }
   ],
   "source": [
    "NB = GaussianNB()\n",
    "clf = NB.fit(train, target)\n",
    "scores = cross_val_score(clf, train, target, cv=10)\n",
    "prec_scores = cross_val_score(clf, train, target, cv=10,scoring=\"precision_weighted\")\n",
    "recall_scores = cross_val_score(clf, train, target, cv=10,scoring=\"recall_weighted\")\n",
    "f1_scores = cross_val_score(clf, train, target, cv=10,scoring=\"f1_weighted\")\n",
    "print(\"Accuracy : \",np.average(scores))\n",
    "print(\"Precision : \",np.average(prec_scores))\n",
    "print(\"Recall : \",np.average(recall_scores))\n",
    "print(\"F1 : \",np.average(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier()\n",
    "clf = decisionTree\n",
    "scores = cross_val_score(clf, train, target, cv=10)\n",
    "prec_scores = cross_val_score(clf, train, target, cv=10,scoring=\"precision_weighted\")\n",
    "recall_scores = cross_val_score(clf, train, target, cv=10,scoring=\"recall_weighted\")\n",
    "f1_scores = cross_val_score(clf, train, target, cv=10,scoring=\"f1_weighted\")\n",
    "print(\"Accuracy : \",np.average(scores))\n",
    "print(\"Precision : \",np.average(prec_scores))\n",
    "print(\"Recall : \",np.average(recall_scores))\n",
    "print(\"F1 : \",np.average(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=0, verbose=1)\n",
    "scores = cross_val_score(clf, train, target, cv=10)\n",
    "prec_scores = cross_val_score(clf, train, target, cv=10,scoring=\"precision_weighted\")\n",
    "recall_scores = cross_val_score(clf, train, target, cv=10,scoring=\"recall_weighted\")\n",
    "f1_scores = cross_val_score(clf, train, target, cv=10,scoring=\"f1_weighted\")\n",
    "print(\"Accuracy : \",np.average(scores))\n",
    "print(\"Precision : \",np.average(prec_scores))\n",
    "print(\"Recall : \",np.average(recall_scores))\n",
    "print(\"F1 : \",np.average(f1_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubah url ke JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArffStr = '''@relation weka-test\\n\\n'''\n",
    "ArffStr = ArffStr + '@attribute url string\\n'\n",
    "ArffStr += '@attribute class {phishing, legitimate}' + \"\\n\\n@data\\n\"\n",
    "\n",
    "for word in data_legitimate:\n",
    "    if('\"' in word):\n",
    "        ArffStr += \"'\"+word+\"',legitimate\\n\"\n",
    "    elif(\"'\" in word):\n",
    "        ArffStr += '\"'+word+'\",legitimate\\n'\n",
    "    else:\n",
    "        ArffStr += \"'\"+word+\"',legitimate\\n\"\n",
    "\n",
    "for word in data_phising:\n",
    "    if('\"' in word):\n",
    "        ArffStr += \"'\"+word+\"',phishing\\n\"\n",
    "    elif(\"'\" in word):\n",
    "        ArffStr += '\"'+word+'\",phishing\\n'\n",
    "    else:\n",
    "        ArffStr += \"'\"+word+\"',phishing\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"url.arff\", \"w\")\n",
    "file.write(ArffStr)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data url ke arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_phising + data_legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_rules = json.loads(open(\"extracted_rules.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "arffStr = convert_for_NLP_with_features(extracted_rules, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@relation weka-test\\n\\n@attribute url string\\n@attribute words string\\n@attribute domain_digit_count numeric\\n@attribute subdomain_digit_count numeric\\n@attribute path_digit_count numeric\\n@attribute domain_length numeric\\n@attribute subdomain_length numeric\\n@attribute path_length numeric\\n@attribute isKnownTld numeric\\n@attribute www numeric\\n@attribute com numeric\\n@attribute punnyCode numeric\\n@attribute random_domain numeric\\n@attribute subDomainCount numeric\\n@attribute char_repeat numeric\\n@attribute - numeric\\n@attribute . numeric\\n@attribute / numeric\\n@attribute @ numeric\\n@attribute ? numeric\\n@attribute & numeric\\n@attribute = numeric\\n@attribute _ numeric\\n@attribute domain_in_brand_list numeric\\n@attribute raw_word_count numeric\\n@attribute splitted_word_count numeric\\n@attribute average_word_length numeric\\n@attribute longest_word_length numeric\\n@attribute shortest_word_length numeric\\n@attribute std_word_length numeric\\n@attribute compound_word_count numeric\\n@attribute keyword_count numeric\\n@attribute brand_name_count numeric\\n@attribute negligible_word_count numeric\\n@attribute target_brand_count numeric\\n@attribute target_keyword_count numeric\\n@attribute similar_keyword_count numeric\\n@attribute similar_brand_count numeric\\n@attribute average_compound_words numeric\\n@attribute random_words numeric\\n@attribute class {phish, legitimate}\\n\\n@data\\n\\'apple-iforget.com\\',\"appleforget\",0,0,0,13,0,0,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,2,1,6.0,7,5,1.0,1,0,1,1,0,0,0,0,7.0,0,phising\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arffStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Hybrid_features.arff\", \"w\")\n",
    "file.write(arffStr)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
